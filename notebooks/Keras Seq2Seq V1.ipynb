{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# standard\n",
    "from IPython import embed\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# frameworks\n",
    "from frameworks.seq2seq_keras.models import AttentionSeq2Seq\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# custom\n",
    "from data_utils import get_train_data\n",
    "from word2vec import get_word_embedding\n",
    "from vocab import get_vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "_BATCH_SIZE = 64\n",
    "_VOCAB_SIZE = 6000\n",
    "_WORD_DIM = 128\n",
    "_MODEL_DEPTH = 4\n",
    "\n",
    "_INPUT_LENGTH = 25\n",
    "_OUTPUT_LENGTH = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = AttentionSeq2Seq(input_length=_INPUT_LENGTH, \n",
    "                         input_dim=_WORD_DIM, \n",
    "                         hidden_dim=_WORD_DIM, \n",
    "                         output_length=_OUTPUT_LENGTH, \n",
    "                         output_dim=_WORD_DIM, \n",
    "                         depth=_MODEL_DEPTH)\n",
    "model.compile(loss='mse', optimizer='rmsprop')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embedding = get_word_embedding(_WORD_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data = get_train_data()\n",
    "_, ch2int = get_vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39956"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pad_to(lst, length, value):\n",
    "    for i in range(len(lst), length):\n",
    "        lst.append(value)\n",
    "    \n",
    "    return lst\n",
    "\n",
    "def clean_train_data(train_data):\n",
    "    X_train = []\n",
    "    Y_train = []\n",
    "    for idx in xrange(len(train_data)):\n",
    "        line_number = idx % 4\n",
    "        \n",
    "        keyword = train_data[idx]['keyword']\n",
    "        current_sentence = train_data[idx]['sentence']\n",
    "        previous_sentences = ''.join([train_data[idx - i]['sentence'] for i in range(line_number, 0, -1)])\n",
    "        \n",
    "        X_entry = pad_to([[ch2int[ch]] for ch in (keyword + previous_sentences)], 25, [_VOCAB_SIZE - 1])\n",
    "        Y_entry = pad_to([[ch2int[ch]] for ch in current_sentence], 10, [_VOCAB_SIZE - 1])\n",
    "        \n",
    "        X_train.append(X_entry)\n",
    "        Y_train.append(Y_entry)\n",
    "        \n",
    "    return X_train, Y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, Y_train = clean_train_data(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_embedded = [map(lambda x: embedding[x[0]], sample) for sample in X_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y_train_embedded = [map(lambda x: embedding[x[0]], sample) for sample in Y_train]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "39956/39956 [==============================] - 421s - loss: 0.4278   \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f834e02d4d0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train_embedded, Y_train_embedded, epochs=1, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "kw = u'山水'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "kw_pad = [pad_to([[ch2int[ch]] for ch in kw], 25, [_VOCAB_SIZE - 1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "kw_embed = [map(lambda x: embedding[x[0]], sample) for sample in kw_pad]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "kw_embed_array = np.array(kw_embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[-0.08963315,  0.13422257, -0.24570994, ..., -0.14995289,\n",
       "          0.17542832,  0.13692029],\n",
       "        [-0.03718845,  0.30552149, -0.17106384, ..., -0.05145008,\n",
       "          0.19576812,  0.19635646],\n",
       "        [-0.08512392,  0.32059655, -0.13894799, ..., -0.04305276,\n",
       "          0.11034762,  0.05403135],\n",
       "        ..., \n",
       "        [ 0.82171226,  0.54636025,  0.8892892 , ..., -0.24688512,\n",
       "         -0.63101834,  0.0330935 ],\n",
       "        [ 0.84401846,  0.55392796,  0.92472196, ..., -0.2636658 ,\n",
       "         -0.64143229,  0.04308706],\n",
       "        [ 0.84558034,  0.54933226,  0.92686731, ..., -0.2670286 ,\n",
       "         -0.64220965,  0.04647564]]], dtype=float32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = model.predict(kw_embed_array)\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w2v_model = Word2Vec.load('data/word2vec.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result = []\n",
    "for i in range(len(pred[0])):\n",
    "    result.append(w2v_model.most_similar(positive=[pred[0][i]], topn=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'result' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-f62339b98eeb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;32mprint\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'result' is not defined"
     ]
    }
   ],
   "source": [
    "for r in result:\n",
    "    print r[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:poetry]",
   "language": "python",
   "name": "conda-env-poetry-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
